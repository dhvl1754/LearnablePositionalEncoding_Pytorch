{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates the implementation of **Learnable Positional Encoding** using PyTorch, integrated into a simple Transformer-based model. A dummy dataset is created to train and evaluate the model. Additionally, we visualize the learned positional encodings and the training loss over epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Importing necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Implementing Learnable Positional Encoding with nn.parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 60, 32])\n"
     ]
    }
   ],
   "source": [
    "class LearnablePositionalEncoding(nn.Module):\n",
    "    \n",
    "    def __init__(self, max_len, d_model):\n",
    "        super().__init__()\n",
    "        # Initialize positional encodings as a learnable parameter\n",
    "        self.pos_encoding = nn.Parameter(torch.zeros(1, max_len, d_model))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pos_encoding[:, :x.size(1), :]\n",
    "\n",
    "d_model = 32  # Dimensionality of the model\n",
    "max_len = 60  # Maximum sequence length\n",
    "pos_encoding = LearnablePositionalEncoding(max_len, d_model)\n",
    "# dummy_input = torch.zeros((1, max_len, d_model))  # Dummy input tensor\n",
    "# output = pos_encoding(dummy_input)\n",
    "# print(f'Output shape: {output.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Creating a Dummy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000, seq_length=10, vocab_size=50):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with random integer sequences.\n",
    "        \n",
    "        Args:\n",
    "            num_samples (int): Number of samples in the dataset\n",
    "            seq_length (int): Length of each sequence\n",
    "            vocab_size (int): Size of the vocabulary (number of unique tokens)\n",
    "        \"\"\"\n",
    "        super(DummyDataset, self).__init__()\n",
    "        self.num_samples = num_samples  # Number of samples in the dataset\n",
    "        self.seq_length = seq_length  # Length of each sequence\n",
    "        self.vocab_size = vocab_size  # Size of the vocabulary (number of unique tokens)\n",
    "        # Generate random integer sequences as dummy data\n",
    "        self.data = torch.randint(0, vocab_size, (num_samples, seq_length))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples #Returns the total number of samples.\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the input-target pair for a given index.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the sample\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (input_sequence, target_sequence)\n",
    "        \"\"\"\n",
    "        return self.data[idx, :-1], self.data[idx, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Setting Up the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 32  \n",
    "seq_length = 10  \n",
    "num_samples = 1000  \n",
    "vocab_size = 50  \n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "dataset = DummyDataset(num_samples=num_samples, seq_length=seq_length, vocab_size=vocab_size)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# for X, Y in dataloader:\n",
    "#     print(f'Input shape: {X.shape}')   \n",
    "#     print(f'Target shape: {Y.shape}')\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Defining the Transformer Model with Learnable Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleTransformerModel(\n",
      "  (embedding): Embedding(50, 32)\n",
      "  (pos_encoding): LearnablePositionalEncoding()\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=32, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=32, bias=True)\n",
      "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc_out): Linear(in_features=32, out_features=50, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class SimpleTransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len, num_heads, num_layers, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the Transformer model with learnable positional encoding.\n",
    "        \n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary\n",
    "            d_model (int): Dimensionality of the model\n",
    "            max_len (int): Maximum sequence length\n",
    "            num_heads (int): Number of attention heads\n",
    "            num_layers (int): Number of Transformer encoder layers\n",
    "            dropout (float): Dropout rate\n",
    "        \"\"\"\n",
    "        super(SimpleTransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)  # Token embedding layer\n",
    "        self.pos_encoding = LearnablePositionalEncoding(max_len, d_model)  # Positional encoding layer\n",
    "        # Transformer encoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)  # Output layer to project back to vocabulary size\n",
    "    \n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)  # (batch_size, seq_length, d_model)\n",
    "        embedded = self.pos_encoding(embedded)  # Add positional encoding\n",
    "        embedded = embedded.permute(1, 0, 2)  # (seq_length, batch_size, d_model) for transformer\n",
    "        encoded = self.transformer_encoder(embedded)  # (seq_length, batch_size, d_model)\n",
    "        encoded = encoded.permute(1, 0, 2)  # (batch_size, seq_length, d_model)\n",
    "        output = self.fc_out(encoded)  # (batch_size, seq_length, vocab_size)\n",
    "        return output\n",
    "\n",
    "# Example usage\n",
    "vocab_size = 50  \n",
    "d_model = 32  \n",
    "max_len = 60  \n",
    "num_heads = 4  \n",
    "num_layers = 2\n",
    "model = SimpleTransformerModel(vocab_size, d_model, max_len, num_heads, num_layers)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 3.9617\n",
      "Epoch 2/10, Loss: 3.8970\n",
      "Epoch 3/10, Loss: 3.8289\n",
      "Epoch 4/10, Loss: 3.7269\n",
      "Epoch 5/10, Loss: 3.6386\n",
      "Epoch 6/10, Loss: 3.5626\n",
      "Epoch 7/10, Loss: 3.4959\n",
      "Epoch 8/10, Loss: 3.4390\n",
      "Epoch 9/10, Loss: 3.3996\n",
      "Epoch 10/10, Loss: 3.3478\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, dataloader, epochs=10, learning_rate=0.001):\n",
    "    #loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    model.train()  \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0  \n",
    "        for X, Y in dataloader:\n",
    "            optimizer.zero_grad()  \n",
    "            output = model(X)  \n",
    "            # Reshape output and target for loss computation\n",
    "            output = output.view(-1, vocab_size)  \n",
    "            Y = Y.view(-1) \n",
    "            loss = criterion(output, Y) \n",
    "            loss.backward() \n",
    "            optimizer.step()  \n",
    "            total_loss += loss.item()  \n",
    "        avg_loss = total_loss / len(dataloader)  \n",
    "        loss_history.append(avg_loss)  \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')  \n",
    "    return loss_history  \n",
    "\n",
    "# Train the model\n",
    "epochs = 10  \n",
    "learning_rate = 0.001  \n",
    "loss_history = train_model(model, dataloader, epochs=epochs, learning_rate=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
